<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>User Agent on </title>
    <link>https://aoii103.github.io/tags/user-agent/</link>
    <description>Recent content in User Agent on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 29 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://aoii103.github.io/tags/user-agent/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>动动手做一个UserAgent库</title>
      <link>https://aoii103.github.io/%E5%8A%A8%E5%8A%A8%E6%89%8B%E5%81%9A%E4%B8%80%E4%B8%AAuseragent%E5%BA%93/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>https://aoii103.github.io/%E5%8A%A8%E5%8A%A8%E6%89%8B%E5%81%9A%E4%B8%80%E4%B8%AAuseragent%E5%BA%93/</guid>
      <description>前言 你是否曾使用着一个两个或者直接百度来谷歌来的几个User-Agent来作为爬虫的UA头，或是从浏览器调试工具扒拉下来的本机UA头，甚至是未使用。。作为一个爬虫脚本，基本的伪装是很必要的。而User-Agent头这个看似不起眼的元素，却起着至关重要的炮灰作用。
当然，这里还是需要科普下UA，是何物。
什么是UA  Wiki：在计算机科学中，用户代理（英语：User Agent）指的是代表用户行为的软件代理程序所提供的对自己的一个标识符。
我：简而言之就是访问者的身份证。
 发现之旅 那么在常规爬虫脚本的编写中，通过每次请求虚假的UA来装饰每次请求，同样能达到一定的伪装作用,较为直观的理解就是，例如戴了个口罩。
 如果对方刨根问底要你摘下口罩查你IP的话，那这个就不适用了，请移步文章[](/img/content_img/ua_demo/)
 废话不多说，说干就干，首先我们打开谷歌（不要问我是怎么打开的。。）并输入user agent list，回车。
这个时候我们可以看到结果中蓝框内有着令人兴奋的结果，UA的基础分类。 而红框内为此结果的源URL地址。于是我们访问这个站点。
我们可以看到列表通过用途进行了一定分类。于是我们进行了一定的对比分析。
https://developers.whatismybrowser.com/useragents/explore/software_name/chrome/ https://developers.whatismybrowser.com/useragents/explore/software_name/internet-explorer/ https://developers.whatismybrowser.com/useragents/explore/operating_system_name/windows/ https://developers.whatismybrowser.com/useragents/explore/operating_platform/nexus-5/ 不难看出，这个站点根据尾部的两级目录进行了分类
\type1\ \type2\ \page  说得直白一点，这或许是 三个循环 就能搞定的事情，当然page得根据抓取结果来判定。
首先我们需要获取到全部的分类链接，也就是上 图中第一个箭头指向元素的 href属性，这里我们通过chrome调试工具直接选取到红框元素css路径并进行对比总结出通用路径
# listing-by-field-name &amp;gt; li:nth-child(1) &amp;gt; h2 &amp;gt; a # listing-by-field-name &amp;gt; li:nth-child(2) &amp;gt; h2 &amp;gt; a # listing-by-field-name &amp;gt; li &amp;gt; h2 &amp;gt; a 并且我们可以发现，下级目录直接给出了二级类型及其总数量，具体取法同上文不再赘述。
我们通过访问任意类型的任一一页，并统计单页数量来直接计算出总页数。那么怎么快速计算呢。这里我们来观察下数据集所处的dom位置。
可以看到全文就一个tbody标签，并且每项的一个td标签存在useragent这样一个类属性。如此就好办了。我们可以通过编写js语句来直接读取单页总数。
于是总页数就为math.ceil(nums/50)
如此一来，一二级目录名称及总页数都找到了解决办法，这个时候我们就可以开始coding了。在编写过程中，我们发现这个站点也是对访问频率进行了一定限制，而Tor代理可以很好地绕过。具体手法详见【url】。
考虑到我们使用的异步模块asks并没有proxy配置项，于是经过一番搜寻，我们找到了一个很好的命令行代理，接下来我给大家介绍它及其使用方法。
命令代理ProxyChain4（以MAC为例）  使用环境Linux, BSD, MAC(win用户的悲剧)</description>
    </item>
    
  </channel>
</rss>